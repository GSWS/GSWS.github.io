<!DOCTYPE html>
<!-- saved from url=(0033)https://cs-people.bu.edu/yizheng/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style>
  #full {
    display: none;
  }
  </style>


  <title>Yi Zheng</title>
  
  <meta name="author" content="Yi Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="./Yi Zheng_files/stylesheet.css">
  <link rel="icon" type="image/png" href="https://cs-people.bu.edu/yizheng/Yi%20Zheng_files/bu.png">
<script type="text/javascript" src="./Yi Zheng_files/jquery.min.js.download"></script></head><div id="photoShowViewer" class="photoShow" style="transition: 0.2s ease-out; opacity: 0;"><div class="photoshow-viewer-shadow" style="transition: 0.2s ease-out;"></div>
        
        <div class="photoshow-img-wrapper">
          <img>
          <div class="photoshow-view-mode-switch-tip">a</div>
        </div>
        
      <i class="photoshow-img-details"></i></div>


<body data-new-gr-c-s-check-loaded="14.1081.0" data-gr-ext-installed="">
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yi Zheng</name>
              </p>
              <p> 
                I am currently a Ph.D. candidate in Computer Science Department at Boston University, advised by Prof. <a href="http://www.cs.bu.edu/~betke/">Margrit Betke</a> and Prof. <a href="https://www.bumc.bu.edu/busm/profile/vijaya-kolachalama/">Vijaya B. Kolachalama</a>. I obtained my master degree in Electrical Engineering at University of Southern California. During my undergraduate study I've interned in Biomedical Imaging Lab led by Prof.<a href="https://www.researchgate.net/scientific-contributions/Boqiang-Liu-2093491922"> Boqiang Liu </a> at the Shandong University. I obtained my bachelor degree in Electrical Engineering from Shandong University, China.</p>
              <p></p>
              <p>
                Before Ph.D. study, I was an algorithm engineer in image quality team in <a href="https://www.gehealthcare.com/en">General Electrics, Healthcare</a>, where I worked on developing image quality tools &amp; algorithms, conducting verification &amp; validation of radiological imaging equipment.
              </p>
              <p>
                I am interested in computer vision, representation learning, and deep learning, I currently focus on representation learning on high-resolution images for tissue phenotyping in computational pathology. It aims at characterizing objective and histopathologic features within gigapixel whole slide images (WSIs) for cancer diagnosis and the estimation of survival time in patients. Unlike CNNs/Vision Transformer which work on downsampled fixed size images, we are dealing with gigapixel images with variable sizes. We extract fine-grained features (such as stroma, tumor cells, lymphocytes), capture local clusters of cell-to-cell interactions (such as tumor cellularity), characterize macro-scale interactions between clusters of cells and their organization in tissue (such as the extent of tumorimmune localization in describing tumor-infiltrating versus tumor-distal lymphocytes), 
                and finally figure out the overall intra-tumoral heterogeneity of the tissue microenvironment depicted at the slide-level of the WSI.

              </p>
              <p style="text-align:center">
                <a href="mailto:yizheng@bu.edu">Email</a> &nbsp;/&nbsp;
                <a href="https://cs-people.bu.edu/yizheng/Yi%20Zheng_files/CV_Yi.pdf">CV</a> &nbsp;/&nbsp;
                <!-- <div class="formswift-button" style="opacity: 0;"><a class="fs-edit-link">✎ Edit</a><a class="fs-sign-link">Sign</a></div>  -->
                <a href="https://scholar.google.com/citations?user=-qUeToQAAAAJ&amp;hl=en&amp;authuser=2"> Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/GSWS"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="./Yi Zheng_files/yizheng.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News &amp; Activities</heading>
            <li style="margin: 5px;">
              2022 BU Computer Science Research Excellence Award (REA), Boston University
            </li>
            <li style="margin: 5px;">
              2019 Orange belt in Karate
            </li>
            <li style="margin: 5px;">
              2018 Annual Boston Dragon Boat Festival Gold medal in Alumni Associations
            </li>
            <p></p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/PAGMIX.svg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph attention-based fusion of pathology images and gene expression for prediction of cancer survival</papertitle>
              <br>
              <strong>Yi Zheng</strong>, Regan D. Conrad, Emily J. Green, Eric J. Burks, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>, Jennifer E. Beane,  <a href="https://sites.bu.edu/vkola/"> Vijaya B. Kolachalama </a>

              <br>
              <em>IEEE Transactions on Medical Imaging (<strong>TMI</strong>) </em>, 2024
              <br>
              <a href="https://cs-people.bu.edu/yizheng/">[Paper]</a> <a href="https://github.com/vkola-lab/TMI2024">[Code]</a>
              <br>
              <p> We present an attention-based fusion architecture that integrates a graph representation of pathology images with gene expression data and concomitantly learns from the fused information to predict patient-specific survival. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/FourierMIL.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FourierMIL: Fourier filtering-based multiple instance learning for whole slide image analysis</papertitle>
              <br>
              <strong>Yi Zheng</strong>, Harsh Sharma, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>, Jennifer E. Beane,  <a href="https://sites.bu.edu/vkola/"> Vijaya B. Kolachalama </a>

              <br>
              <em>Under review, 2024
              <br>
              <a href="https://cs-people.bu.edu/yizheng/">[Paper]</a> <a href="https://github.com/vkola-lab/ECCV2024">[Code]</a>
              <br>
              <p> We present an attention-based fusion architecture that integrates a graph representation of pathology images with gene expression data and concomitantly learns from the fused information to predict patient-specific survival. </p>
            </em></td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/TMI.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>A deep learning based graph-transformer for whole slide image classification</papertitle>
              <br>
              <strong>Yi Zheng</strong>, Rushin Gindra, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>, Jennifer E. Beane,  <a href="https://sites.bu.edu/vkola/"> Vijaya B. Kolachalama </a>

              <br>
              <em>IEEE Transactions on Medical Imaging (<strong>TMI</strong>) </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2205.09671">[Paper]</a> <a href="https://github.com/vkola-lab/TMI2022">[Code]</a>
              <br>
              <p> We present a Graph-Transformer (GT) based framework for processing pathology data, called GTP, that interprets morphological and spatial information at the WSI-level to predict disease grade. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/ICIP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic-based Sentence Recognition in Images using Bimodal Deep Learning</papertitle>
              <br>
              <strong>Yi Zheng</strong>, <a href="https://scholar.google.com/citations?user=pZ0uHngAAAAJ&amp;hl=en"> Qitong Wang</a>, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>

              <br>
              <em>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/1908.01403.pdf">[Paper]</a> <a href="https://github.com/ivc-yz/SSR">[Code]</a> <a href="https://drive.google.com/drive/folders/1DxOau_B1UJewj2xSKYyIS2XHa2BYbCj2?usp=sharing">[Dataset]</a>
              <br>
              <p> Semantic-based Sentence Recognition (SSR) can efficiently understand the context between regions of text or between words in images by extracting sentences or paragraphs from images instead of only isolated text regions or words.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/AJPA.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep learning driven quantification of interstitial fibrosis in digitized kidney biopsies</papertitle>
              <br>
              <strong>Yi Zheng</strong>, Clarissa A. Cassol, Saemi Jung, Divya Veerapaneni, Vipul C. Chitalia, Kevin Ren, Shubha S. Bellur, Peter Boor, Laura M. Barisoni, Sushrut S. Waikar, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>, <a href="https://sites.bu.edu/vkola/"> Vijaya B. Kolachalama </a>

              <br>
              <em>The American Journal of Pathology (<strong>AJPA</strong>)</em>, 2021
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S000294402100208X?via%3Dihub">[Paper]</a> <a href="https://github.com/vkola-lab/ajpa2021">[Code]</a>
              <br>
              <p> Our framework to analyzing microscopic- and WSI-level changes in renal biopsies attempts to mimic the pathologist and provides a regional and contextual estimation of IFTA. Such methods can assist clinicopathologic diagnosis. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/LAL.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LAL: Linguistically Aware Learning for Scene Text Recognition</papertitle>
              <br>
              <strong>Yi Zheng</strong>, <a href="https://cs-people.bu.edu/wdqin/"> Wenda Qin</a>,   <a href="https://derrywijaya.github.io/web/"> Derry Wijaya </a>, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>

              <br>
              <em>ACM MULTIMEDIA CONFERENCE (<strong>ACMMM</strong>)</em>, 2020
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413913">[Paper]</a> <a href="https://github.com/ivc-yz/LAL">[Code]</a>
              <br>
              <p> Linguistically Aware Learning (LAL) scene text recognizer is a a bimodal framework that simultaneously utilizes visual and linguistic information to enhance scene text recognition performance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/CVPRW.JPG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>A method for detecting text of arbitrary shapes in natural scenes that improves text spotting</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=pZ0uHngAAAAJ&amp;hl=en"> Qitong Wang</a>, <strong>Yi Zheng</strong>, <a href="https://www.cs.bu.edu/faculty/betke/"> Margrit Betke </a>

              <br>
              <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<strong>CPPRW</strong>)</em>, 2020
              <br>
              <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=pZ0uHngAAAAJ&amp;citation_for_view=pZ0uHngAAAAJ:9yKSN-GCB0IC">[Paper]</a> <a href="http://www.cs.bu.edu/faculty/betke/UHT.">[Code]</a>

              <br>
              <p> UHT, short for UNet, Heatmap, and Textfill, uses a UNet to compute heatmaps for candidate text regions and a textfill algorithm to produce tight polygonal boundaries around each word in the candidate text. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/Kenya.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Scraping social media photos posted in Kenya and elsewhere to detect and analyze food types</papertitle>
              <br>
              Mona Jalal, Kaihong Wang, Sankara Jefferson, <strong>Yi Zheng</strong>, Elaine O Nsoesie, Margrit Betke

              <br>
              <em>Proceedings of the 5th International Workshop on Multimedia Assisted Dietary Management</em>, 2019
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3347448.3357170">[Paper]</a> <a href="https://cs-people.bu.edu/yizheng/">[Code]</a>

              <br>
              <p> We propose a scrape-by-location methodology to create food image datasets from Instagram posts. We applied our techniques to the millions of Instagram posts we collected across Kenya over a period of 20 days to give an
                example of the kind of analysis social scientists may conduct with our tools. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/rushin.jfif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Histological profiling of lung premalignant lesions and tumors using graph convolutional networks</papertitle>
              <br>
              Rushin Gindra, <strong>Yi Zheng</strong>, Vijaya B. Kolachalama, Jennifer E. Beane 

              <br>
              <em>NCI Informatics Technology for Cancer Research (ITCR),2022 (Under Review) </em>
              <br>
              <a href="https://cs-people.bu.edu/yizheng/">[Paper]</a> <a href="https://cs-people.bu.edu/yizheng/">[Code]</a>

              <br>
              <p> We developed a graph convolutional network that can efficiently process WSIs of hematoxylin and eosin (H&amp;E)-stained NAT and tumor tissues from lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD) cases.our analysis identified unique WSI-level signatures of NAT for LSCC and NAT for LUAD, which are distinct from those obtained on tumor WSIs. These findings underscore the importance of characterizing NAT samples in the context of their proximity to tumor subtype, and such phenotyping can facilitate studies on histologically precancerous lesions. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="./Yi Zheng_files/Lindsey.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Computational assessment of early diabetic nephropathy</papertitle>
              <br>
              Lindsey Claus, Yichi Zhang, <strong>Yi Zheng</strong>, Tejus Surendan, Vipul Chitalia, Patrick Walker, Clarissa Cassol, Vijaya B. Kolachalama 

              <br>
              <em> <strong>Under Review</strong> </em>
              <br>
              <a href="https://cs-people.bu.edu/yizheng/">[Paper]</a> <a href="https://cs-people.bu.edu/yizheng/">[Code]</a>

              <br>
              <p> A deep learning framework known as a feature pyramid network (FPN) was implemented to classify digitized renal biopsies as class I or II DN. Our study identified several regions on the biopsy images as informative for prediction of class I vs II DN. Further analysis can elucidate the importance of various histopathological features of early stage DN.
              </p>
            </td>
          </tr>
          
      
    
  </tbody></table>
 
<p></p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <a href="https://clustrmaps.com/site/1bmyt" title="Visit tracker"><img src="./Yi Zheng_files/map_v2.png"></a>
	  </div>        
	  <br>
	    © Yi Zheng | Last updated: Jan 21, 2022
</center><p></p>



<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></td></tr></tbody></table></body></html>